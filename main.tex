\documentclass[10pt,twocolumn]{article}
\usepackage{graphicx}    % For including images
\usepackage{amsmath}     % For mathematical symbols
\usepackage{caption}     % Better captions
\usepackage{float}       % Precise float placement
\usepackage{geometry}    % To adjust margins
\geometry{margin=1in}
\usepackage[backend=biber,style=apa]{biblatex}
\addbibresource{references.bib}  % This is your .bib file name

\title{\textbf{Machine Learning-Based Classification of Table Tennis Swings Using Racket Kinematics}}
\author{Jevi Waugh \\
Faculty of Engineering, Architecture and Information Technology \\
COMP4702 - Machine Learning}
\date{May 2025}

\begin{document}

\maketitle
% START OF PAGE 1
\begin{abstract}
This report investigates the application of machine learning models to classify the demographics of table swings based on racket kinematics data, with a strong focus on predicting a combined age and gender label. The Data Set originally sourced from DRYAD has slightly been modified for the purpose of this report. We apply and compare three supervised classification algorithms, K-Nearest Neighbours (KNN), Support Vector Machines (SVM) and One-vs-Rest Logistic Regression, evaluating their performance using known evaluation metrics. Our findings provide insights into the discriminative power of racket motion features and the behaviour of various classifiers on a real-word problem. 
\end{abstract}
%
\section{Introduction}
Human motion analysis and dexterity is a core area of research in sports, particularly in table tennis. In the context of table tennis, it is important to understand how swing patterns relate to certain demographic attributes such as age and gender, which can inform training strategies and biomechanical insights such as angular velocity, acceleration, power spectral density, etc. This report explores the use of supervised machine learning models to classify a composite label consisting of age and gender from the table tennis-racket-swing dataset. Pre-Processing will be required to ensure that valid numerical data are utilised efficiently. The considered ML models: K-Nearest Neighbor (K-NN), logistic regression and kernel based methods (such as a support vector machine) will be used. The purpose of this report is not to only maximise accuracy and lower $E_{new}$, but to primarily make a critical evaluation of each model's behaviour, generalisation capability, feature processing, and sensitivity to hyper-parameters.

\section{Exploratory Data Analysis}
The provided dataset had some irregularities in terms of feature values and also had other inconsistencies with our classification task. The features that were dropped are 'id' and 'date' primarily because ID identifies and metadata impertinent to the player kinematics will strictly skew the classification workflow as they are strictly not-relevant to the problem.
\subsection{Encoding and Normalisation}
Our classification problem will make use one-hot-encoding for features in Table~\ref{tab:cat-values} excluding "???".  Age encodes,  low $\rightarrow$ 0, medium $\rightarrow$ 1, high $\rightarrow$ 2. This provides us the following 6 labels as shown in Table~\ref{tab:encoding-scheme}. 
We then represent these modalities as low $\rightarrow$ Young, medium $\rightarrow$ Mid-age, high $\rightarrow$ Older.
For gender, we have binary numerical values which we interpret as, 0 $\rightarrow$ Female, and 1 $\rightarrow$ Male. 
\footnote{Note that Gender is interpreted, not encoded; age is encoded once, then interpreted via standard terms.}
\begin{table}[H]
\centering
\caption{Encoding scheme for combined gender and age classification}
\begin{tabular}{|l|}
\hline
\textbf{Combined Labels} \\
\hline
00 $\rightarrow$ Young Female \\
10 $\rightarrow$ Young Male \\
01 $\rightarrow$ Mid-age Female \\
11 $\rightarrow$ Mid-age Male \\
02 $\rightarrow$ Older Female \\
12 $\rightarrow$ Older Male \\
\hline
\end{tabular}
\label{tab:encoding-scheme}
\end{table}
The dataset will be either normalised using the formula \begin{equation}x_{ij}^{new} = {\dfrac {x_{ij} - min_l{(x_{lj})}} {max_l{(x_{lj})} - min_l{(x_{lj})}}}\label{eq:minmax}
\end{equation} or standardised, using   $Z={\dfrac {\bar{X} - \mu} {\sigma}}$depending on the model's sensitivity to feature scaling and the distribution of the data.
\subsection{Dataset description and assumptions}
\label{subsec:dataset-description-and-assumptions}
The dataset has 97,355 data-points \footnote{Modified data from source \cite{dryad_tabletennis2021}.} and 49 features excluding 'id' and 'date'. There were other inconsistencies such as missing values labelled as "???" in certain columns (see Table~\ref{tab:cat-values}). There were 5 missing data points in each random variable (feature), 20 values, a total of which their rows were dropped. We also assume that the dropped rows won't affect prediction accuracy given that its only $5.13 \times 10^{-5}$\% of the entire data set.

\begin{table}[H]
\centering
\caption{Categorical values (before cleaning)}
\begin{tabular}{|l|l|}
\hline
\textbf{Feature} & \textbf{Unique Values} \\
\hline
Height     & \texttt{high, low, medium, ???} \\
Age        & \texttt{high, medium, ???, low} \\
Play Years & \texttt{high, medium, ???, low} \\
Weight     & \texttt{high, medium, low, ???} \\
\hline
\end{tabular}
\label{tab:cat-values}
\end{table}
Furthermore, an analysis on class imbalance was made to ensure full interpretability when we make predictions. We can see that in Figure~\ref{fig:stacked-results} gender has a class imbalance of 60-40\%.To address the class imbalance in gender, we will use the stratify parameter in Scikit-learn's \texttt{train\_test\_split}
 function to ensure that proportion of each class is preserved in both the training and test sets. Without stratification, random splitting of the data might over-represent one class leading to a biased model. While this imbalance is not as extreme as  presented ones such as Figure~\ref{fig:stacked-hand-results}, this ensures a much more accurate model. \texttt{Handedness} and \texttt{holdRacketHanded} both have an exact imbalance of 83-16\%. We will address this later during feature selection. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{images/gender.png}
    \caption{Gender Imbalance}
    \label{fig:stacked-results}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{images/hand_data.png}
    \caption{Hand data Imbalance}
    \label{fig:stacked-hand-results}
\end{figure}

We also observe the distributions of each numerical feature in the figures in the appendix. While some features exhibit skewness and uniformity, many including \texttt{newv1}, \texttt{newv2} and \texttt{g\_max} appear to follow a roughly Gaussian distribution. Although strict normality cannot be assumed without some sort of formal statistical testing, we proceed under the simplifying assumption that the data is normally distributed.  The Central Limit Theorem suggests that convergence to a Gaussian distribution as $n \rightarrow \infty $, even when the underlying data is not perfectly Gaussian.\footnote{For intuition, imagine the histogram having smaller bin widthsâ€”this would clarify whether the distribution approximates a known form.}
 This is considered a reasonable assumption for the purposes of standardisation and model compatibility.

Before applying any machine learning model, we will identify potential redundancy among features by computing a Pearson correlation matrix as seen in Figure~\ref{fig:correlation-matrix}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{images/correlation_matrix.png}
    \caption{Pearson's correlation matrix}
    \label{fig:correlation-matrix}
\end{figure}
We can see that the acceleration, velocity and gyro-signal features are heavily correlated. Highly correlated features can introduce multicollinearity which can negatively impact a model that assumes feature independence (such as Logistic regression). Dropping highly correlated features will be further explored in other models to evaluate its effectiveness and generalisation.
% START OF PAGE 2
\section{Model 1: K-Nearest Neighbours (KNN)}
We will first explore K-Nearest neighbour as it's a non-parametric model that uses the euclidean distance metric. We will also discuss how normalisation and this distance metric simultaneously affect the accuracy.
\subsection{Model Training and Hyper-parameter tuning}

From an experimentation perspective, we will normalise instead of standardise to see how the model operates disregarding our initial assumption about the distribution of the data. First we train an initial model with \texttt{K=1, K=3}, requiring approximately 5 minutes of computation time during model training and with an 80/20 train-test split, reserving 20\% of the data for testing. This resulted into a classification accuracy of 53\% and 63\% respectively. Empirically speaking, we observed (based on the distribution of the random variables) that when features are skewed or contain outliers, normalisation compresses the majority of the data into a narrow range, which distorts the euclidean calculations and leads to reduced performance. This is another reason why our model focuses more on using the $z-score$ while normalisation fails Equation~\ref{eq:minmax}.

Now suppose we standardise as we are meant to, which reduces the effect of differing feature scales and we apply the same conditions as before but for different $k$ values as outlined in Figure~\ref{fig:knn-k}.


\subsection{Accuracy and classification}
After model training and finding the optimal $k$ value based on the lowest $E_{new}$ in Figure~\ref{fig:knn-k}, we see high performance across all classes, with precision, recall, and F1-scores averaging around 96-97\%. The accuracy for training was 98\% while the test data set yielded 96\%. The confusion matrix in Figure~\ref{fig:knn-cm} confirms this, displaying strong diagonal dominance, especially for the two most accurate class 1-0 (Young Male) and 1-2 (Older Male), which achieved 1588 out of 1640, and 5849 out of 6060 correct predictions respectively. Minor off-diagonal entries suggest that misclassifications tend to occur between adjacent classes, likely due to overlapping feature distributions (e.g., confusion between 0-0 and 0-2 or between 1-1 and 1-2). This is expected in a demographic classification task where class boundaries may be gradual. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{images/knn/knn cm.png}
    \caption{KNN Confusion Matrix}
    \label{fig:knn-cm}
\end{figure}
Analysing scores in lower support classes such as class 1 (1320 samples) and class 3 (1640 samples) indicate that the model is not overfitting to the majority classes. Moreover, if we look deeper, there are about 70 instances (in Figure~\ref{fig:knn-cm}) of class 1-1 (Mid-age Male) confused with 1-2 (Older Male). Our argument to this is the inherent ambiguity in defining discrete age categories based on continuous physiological behaviour. There is no clear margin for what constitutes "mid-age" and "older", and swing kinematics in terms of angular velocity in all axes when using the racket could be very similar. For example, if the age cutoff for "mid-age" is 55 and the and "Older" began at 56, then individuals close to that threshold would likely produce nearly indistinguishable kinematic patterns. This could very well result in a very similar feature distribution. Hence, while class "medium" and "high" may signify the age of an individual, the model does not take into account, that they could potentially represent similar feature patterns near boundary ages.
%
\begin{table}[H]
\centering
\small  % Reduce font size
\caption{Classification report for 6-class model}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccc}
\hline
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Support} \\
\hline
0 & 0.96 & 0.95 & 0.95 & 1990 \\
1 & 0.94 & 0.95 & 0.95 & 1320 \\
2 & 0.96 & 0.97 & 0.97 & 4400 \\
3 & 0.99 & 0.97 & 0.98 & 1640 \\
4 & 0.96 & 0.96 & 0.96 & 4060 \\
5 & 0.97 & 0.97 & 0.97 & 6060 \\
\hline
\textbf{Accuracy} & \multicolumn{3}{c}{0.96} & 19470 \\
\textbf{Macro Avg} & 0.96 & 0.96 & 0.96 & 19470 \\
\textbf{Weighted Avg} & 0.96 & 0.96 & 0.96 & 19470 \\
\hline
\end{tabular}
}
\label{tab:classification-report}
\end{table}

In Table~\ref{tab:classification-report}, we can see the Macro avg and weighted all around around 0.96 suggesting consistent performance and reliable generalisation, given class imbalance present in the dataset was handled.
% START OF PAGE 3
\clearpage  % or \newpage
The Figure~\ref{fig:knn-k} illustrates the test error $E_{new}$ across various $k$ values in the classifier. This was done iteratively with random sampling and tuning this $k$ hyper-parameter really showed certain drops in accuracy although the differences were not more than 0.01. A sharp drop is observed at $k=3$, which achieves the lowest test error 3.65\%, along with $k=5$ suggesting a good balance between bias and variance since their training accuracies were both 98\% which is close to 2\% higher than their test accuracy 96\%.  In this case, we choose $k=3$.
\subsubsection{$E_{new}$ evaluation}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{images/knn/knnn k's.png}
    \caption{$E_{new}$ for each $k$}
    \label{fig:knn-k}
\end{figure}
As $K$ increases, the errors fluctuates but remain relatively low, suggesting that the model is stabilising beyond $k=3$. This supports the idea that selecting an appropriate $K$ value is crucial as very small $k$ or very large $k$ can affect the model's performance. Furthermore, the magnitude of $K$ is not the only thing that affects the classifier, but considering whether $K$ is odd or even. Using an odd $K$ helps reduce the likelihood of tied votes when classifying. Although ties are less common in a six-class problem compared to binary classification, they can still occur, especially when boundary samples are surrounded by neighbours from different classes. Figure~\ref{fig:knn-k} is an illustration of this effect where every even $k$ value is higher than the odd values next to it.
\subsection{Evaluation Metrics and Results}
We will perform a deeper evaluation using an ROC curve and the learning curves of training and testing through different batches of the hold-out validation data.WRITE MORE
\subsubsection{ROC Curves}
The ROC curve in Table~\ref{fig:knn-roc} was computed using the one-vs-rest approach for each class showing perfect AUC scores of 0.99 across almost all classes except class 1-0 which achieves 1.0. While this looks suspicious, let's understand that this has a connotation of KNN's strong ability to differentiate between classes, it is important to note that these high score only measure how well the model separates the classes and not whether it assigns the correct label and may still misclassify samples when forced to make difficult predictions as outlined on page 3 when classifying between "Mid-age Male" and "Older Male" etc...
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{images/knn/roc curve.png}
    \caption{KNN ROC Curve.}
    \label{fig:knn-roc}
\end{figure}

Although stratified sampling was used, class imbalance remains a fundamental characteristic. When evaluating using one-vs-rest ROC curves, larger classes such as 1-2 in Table~\ref{tab:combined-class-distribution} dominates the score computations and classifiers generally do better if they have more examples to learn from. In terms of the AUC bias, it seems that for KNN's result that, it's easier to get a high AUC for large classes, regardless if the precision, F1 or even the recall metric is worse for classes with less examples.

\begin{table}[H]
\centering
\caption{Distribution of Combined Gender-Age Classes}
\begin{tabular}{|c|c|}
\hline
\textbf{Class (Gender-Age)} & \textbf{Count} \\
\hline
1-2 (Maleâ€“High)   & 30,300 \\
0-2 (Femaleâ€“High) & 22,000 \\
1-1 (Maleâ€“Medium) & 20,300 \\
0-0 (Femaleâ€“Low)  & 9,950 \\
1-0 (Maleâ€“Low)    & 8,200 \\
0-1 (Femaleâ€“Medium) & 6,600 \\
\hline
\end{tabular}
\label{tab:combined-class-distribution}
\end{table}
It is critical to make the observation that there are significantly less young people than "mid-age" and "older" people.

\clearpage  % or \newpage
\subsubsection{Cross validation and learning curve}
We do another form of experimentation to see if our previous results hold, by strictly evaluating the performance of KNN with an initial 5-fold cross in Table~\ref{tab:cv-scores} with $k=3$ across the entire dataset. The resulting accuracy scores showed moderate variance across folds, with a mean accuracy of approximately 54.7\%.This is not ideal but we understand that this is also due to the fact that there was no \texttt{stratify} parameter in sci kit-learn's \texttt{cross\_val\_score} function not directly handling the class (combined label) imbalance.
\begin{table}[H]
\centering
\caption{5-Fold Cross-Validation Accuracy Scores}
\begin{tabular}{lc}
\hline
\textbf{Fold} & \textbf{Accuracy} \\
\hline
1 & 0.572 \\
2 & 0.626 \\
3 & 0.628 \\
4 & 0.478 \\
5 & 0.431 \\
\hline
\textbf{Mean} & \textbf{0.547} \\
\hline
\end{tabular}
\label{tab:cv-scores}
\end{table}
While KNN evaluates each fold, it's strongly suspected that these accuracies are strictly modest due to the concentration of similar-class samples within individual folds.  This is problematic as we can see in Table~\ref{tab:combined-class-distribution}, classes such as 1-2 dominate while 0-1 are underrepresented. This limited diversity significantly lowers $E_{hold-out}$ in each fold reducing the model's ability to generalise.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{images/knn/learning rate and cross validation.png}
    \caption{KNN Learning curve}
    \label{fig:knn-lrate}
\end{figure}
We analyse the learning curve of KNN generated using 5-fold cross validation providing increase in accuracy for varying training sizes. For training accuracy, it's not surprising to see KNN memorising the data resulting in 100\%. However the validation accuracy for each training batch plateaus around 55\% even though it increases every time the training size increases. Although this gradual rise indicates that the model benefits from more data, the persistent gap between training and validation performance shows that there is an immense amount of high variance and overfitting. Unlike the previous split cross validation approach shown in Table~-\ref{tab:cv-scores}, this cross-validated method approach offers a more robust estimate of KNN's true capabilities as a model that primarily looks at local neighbourhood structures in the data. Since it has a big generalisation gap, it's fair to say that KNN struggles to generalise well in high-dimensional, imbalanced multi-classification problems. 

This conclusion is made primarily because of KNN's inherent sensitivity to class imbalance class imbalance and because hold-out cross-validation forces the model to generalise over all parts of the data in terms of varying class distributions.  A more complex model is required to improve generalisation for our high-dimensional dataset.

% START OF PAGE 4
% \clearpage  % or \newpage
\section{Model 2: Logistic regression}
Unlike KNN, logistic regression is a parametric model that learns a set of weights $\theta$ to separate classes using linear decision boundaries using the logistic loss~\ref{eq:logistic-loss}

\subsection{Solver and regularisation analysis}
Training logistic regression took a significant amount of time (1H+) to compute and we had the same initialisations as KNN in terms of test size, standardisation and stratification. Three models were trained each with different solvers and regularisation  techniques. LIBLINEAR +L1 performed slightly better at 71.55\% followed closely by LBFGS solver + L2 (71.53\%) and lastly LIBLINEAR + L2 (71.47\%). This shows that the solvers and regularisation had little impact on the predictive accuracy for classifying gender and age combined. While L1 is more desirable, since it's meant to offer sparsity, this effect was not evident in the top feature coefficients shown in Table~\ref{tab:logreg-liblinear-l1} in compared to L2 as shown in Figure~\ref{tab:logreg-lbfgs-l2} and Figure~\ref{tab:logreg-liblinear-l2}.
\begin{table}[H]
\centering
\footnotesize
\caption{Top 2 Features per Class â€” LIBLINEAR + L1 (Rounded Coefficients)}
\begin{tabular}{|c|p{6.5cm}|}
\hline
\textbf{Class} & \textbf{Top Features (Coefficient)} \\
\hline
0 & \texttt{height\_high} (-4.80), \texttt{a\_mean} (1.75) \\
1 & \texttt{playYears\_high} (-3.96), \texttt{height\_high} (-3.54) \\
2 & \texttt{a\_mean} (2.25), \texttt{ax\_rms} (-2.01) \\
3 & \texttt{height\_high} (13.15), \texttt{weight\_low} (4.89) \\
4 & \texttt{playYears\_high} (-3.52), \texttt{ax\_rms} (1.97) \\
5 & \texttt{ax\_mean} (-1.72), \texttt{a\_mean} (-1.64) \\
\hline
\end{tabular}
\label{tab:logreg-liblinear-l1}
\end{table}
\clearpage
Examining the coefficients, for example class 3 (1-0), \texttt{height\_high} has a very large positive coefficient meaning that it can strongly identify that class. \texttt{playYears\_high} and \texttt{height\_high} have strong negative coefficients, reducing the likelihood of those classes when those features are present. So logistic regresssion is very intentional when classifying.
\subsection{Confusion matrix}
Analysing the confusion matrix, it still does a remarkable job at classifying but only 71.55\% of the time. We can see that the model is effective at differentiating between the classes but struggles with several specific class boundaries. For instance class 0-0 and 0-2 show significant confusion, with 681 instances of Young female being misclassified as older female. Similarly, middle-aged men (1-1) is frequently confused with older men (1-2). Indicating that the model finds it incredibly difficult to separate adjacent group within the same gender. Nonetheless, it's evident to see that the model does a satisfactory job at capturing the broad structure of the dataset.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/lr/cm-reduced-data.png}
    \caption{Confusion Matrix (liblinear + L1)}
    \label{fig:cm-liblinear-l1}
\end{figure}
This shows that utilising a model, using different hyper-parameters and solvers is not enough. Another experiment is to select certain features via correlation which is explored in the next section. 
\subsection{Feature selection and correlation}
Earlier we discussed the correlation matrix, putting that to use is extremely critical for logistic regression since it dropping features will be key for the model to perform better.
The original correlation matrix in Figure~\ref{fig:correlation-matrix} showed several clusters particular around features such as velocity and gyroscope data, which can introduce multicollinearity and stop the model to be interpretable especially in terms of its coefficients. The dropped features were \texttt{[ax\_rms, ay\_rms, gx\_rms, gy\_rms, gz\_rms, a\_ma, a\_mean, g\_fft, g\_psdx, g\_entropy, holdRacketHanded\footnote{Note that we previously discussed how holdRacketHanded was an extremely imbalanced feature in Figure~\ref{fig:stacked-hand-results} and now it has been dropped.}]} 

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/lr/reduced-correlation-matrix.png}
    \caption{Reduced correlation matrix}
    \label{fig:reduced-correlation-matrix}
\end{figure}
After applying a correlation threshold of $p=0.9$, the new correlation matrix shows a much clearer relationship and the big cluster has now been reduced to a smaller cluster with only one diagonal instead of 3 diagonal lines. 
Finally we train out model again, but this time, we change the train-test-split to 90-10\%. Table~7 outlines the new accuracy for our logistic model. This time out accuracy is 73.2\% which is a 1.65\% increase. 
\begin{table}[H]
\centering
\caption{Classification Report for Logistic Regression (LIBLINEAR + L1)}
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
\hline
0 & 0.655 & 0.444 & 0.529 & 1990 \\
1 & 0.615 & 0.403 & 0.487 & 1320 \\
2 & 0.729 & 0.804 & 0.764 & 4400 \\
3 & 0.890 & 0.891 & 0.891 & 1640 \\
4 & 0.643 & 0.750 & 0.692 & 4060 \\
5 & 0.794 & 0.790 & 0.792 & 6060 \\
\hline
\textbf{Accuracy} & \multicolumn{4}{c|}{0.732} \\
\textbf{Macro Avg} & 0.721 & 0.680 & 0.693 & 19470 \\
\textbf{Weighted Avg} & 0.730 & 0.732 & 0.726 & 19470 \\
\hline
\end{tabular}
}
\label{tab:logreg-classification-report}
\end{table}
It seems to be doing well on class 3 and class 5 but performs poorly on class 0 and 1. We can also see that the support for class 0 and 1 are significantly less compared to class 3 and 5 which have higher than 4000 samples. The reason why class 3 and 5 are high-performing classes is due to the fact that logistic regression relies on learning class-specific weights using the available data. This multi-classification logistic model uses SoftMax which will inherently favour majority classes. The recall for class 0 and 1 is notably low indicating that the model failed to correctly identify more than half of the actual instances from these classes.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/lr/cm-liblinear-l1-normal-training.png}
    \caption{Confusion Matrix (features dropped)}
    \label{fig:cm-reduced-data}
\end{figure}
Nonetheless comparing Figure~\ref{fig:cm-liblinear-l1} with Figure~\ref{fig:cm-reduced-data}, we can see the increase of TPR across the diagonal. This shows that feature correlation from the dataset does have an impact on the accuracy of the model but in this experiment, it's not significant.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/lr/roc-liblinear-l1-normal-training.png}
    \caption{ROC (features dropped)}
    \label{fig:roc-reduced-data}
\end{figure}
For a much deeper inspection, The ROC shows strong class separation with AUC score above 0.88 for all classes. Class 1-0 stands out with an AUC of 1.00, which indicates flawless ranking for that particular class. class (1-2) shows the lowest score of 0.89. Despite high AUC values, these results must be interpreted alongside recall and accuracy, as high ranking performance does not always translate to correct predictions in a one-vs-rest strategy. Inspecting the false positive rate, we can see that it's significantly higher KNN's ROC curve in Figure~\ref{fig:knn-roc}.
\subsubsection{Cross validation on reduced correlation}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/lr/cross-validation - reduced.png}
    \caption{Learning curve (features dropped)}
    \label{fig:lr-learning-curve}
\end{figure}
To find the true capabilities of the model, we cross-validate with a stratified 4-fold with the liblinear solver and L2 regularisation on the reduced low-correlation feature dataset, which initially illustrates a high variance with the training performance being 100\%. As the training sizes gets larger, we can see the generalisation gap getting smaller and smaller, with the training accuracy decreasing and validation increasing, showing moderate bias and some variance plateauing around $E_{hold-out}=57$\%. This shows on average, the model struggles due to the complexity and overlap of class boundaries in the feature space, especially for minority classes. The model is not overfitting nor underfitting, it assumes a linear relationship between features and log-odds of classification, which might be too simplistic for capturing the nuanced decision surfaces (non-linear features) needed to separate six combined gender-age classes. Furthermore, we understand that switching from a non-parametric model (KNN) to a parametric model such as logistic regression that has the capability to learn weights and tune different hyperparameters, does not necessarily make a better model nor achieve a higher accuracy due to the mechanics of the individual model and loss functions used.

\clearpage
\section{Model 3: Support Vector Machine (SVM)}
To address the limitations of logistic regression, we will look at kernel based methods where we can increase the dimensionality of the model to capture much more complex patterns in the data.

\subsection{Kernel and regularisation exploration}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/svm/svm-linear-vs-kernel-and-regularisation.png}
    \caption{Enter Caption}
    \label{fig:svm-kernels}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/svm/svm-cm-linear-vs-kernel-and-regularisation.png}
    \caption{Enter Caption}
    \label{fig:svm-cm-kernels}
\end{figure}

\subsection{Evaluation Metrics and Results}

\begin{table}[H]
\centering
\caption{Classification Report (Accuracy = 77.97\%)}
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
\hline
0 & 0.74 & 0.62 & 0.68 & 1990 \\
1 & 0.62 & 0.43 & 0.51 & 1320 \\
2 & 0.76 & 0.80 & 0.78 & 4400 \\
3 & 0.94 & 0.87 & 0.90 & 1640 \\
4 & 0.69 & 0.86 & 0.76 & 4060 \\
5 & 0.87 & 0.81 & 0.84 & 6060 \\
\hline
\textbf{Accuracy} & \multicolumn{4}{c|}{0.780} \\
\textbf{Macro Avg} & 0.77 & 0.73 & 0.75 & 19470 \\
\textbf{Weighted Avg} & 0.78 & 0.78 & 0.78 & 19470 \\
\hline
\end{tabular}
}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/svm/svm-roc-linear-vs-kernel-and-regularisation.png}
    \caption{Enter Caption}
    \label{fig:svm-roc-kernels}
\end{figure}


\label{tab:classification-77-accuracy}
\end{table}


\subsection{Training on non-feature selection}


This time we will explore deeper by training on the original dataset which is highly correlated among some features and we will use a different kernel.

\begin{table}[H]
\centering
\caption{Classification Report for Best Performing Model}
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
\hline
0 & 0.929 & 0.882 & 0.905 & 1990 \\
1 & 0.963 & 0.924 & 0.943 & 1320 \\
2 & 0.950 & 0.939 & 0.945 & 4400 \\
3 & 0.981 & 0.958 & 0.969 & 1640 \\
4 & 0.913 & 0.956 & 0.934 & 4060 \\
5 & 0.946 & 0.954 & 0.950 & 6060 \\
\hline
\textbf{Accuracy} & \multicolumn{4}{c|}{0.942} \\
\textbf{Macro Avg} & 0.947 & 0.936 & 0.941 & 19470 \\
\textbf{Weighted Avg} & 0.942 & 0.942 & 0.942 & 19470 \\
\hline
\end{tabular}
}
\label{tab:svm-poly-report}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/svm/svm-cm-poly.png}
    \caption{Enter Caption}
    \label{fig:svm-poly-cm}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/svm/svm-poly-roc.png}
    \caption{Enter Caption}
    \label{fig:svm-roc-poly}
\end{figure}


\clearpage
\section{Comparative Analysis}


\subsection{Summary Table of Model Performance}

\subsection{Bias variance trade off}

\subsection{Feature Sensitivity and interpretability}


\section{Conclusion}


\subsection{Summary of Findings}


\subsection{Insights on applied models}



\printbibliography

\clearpage  % or \newpage
\appendix
\section*{Appendix}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth, height=0.9\textheight]{images/distribution_1.png}
    \caption{Distributions of features (1 of 3).}
    \label{fig:distribution-1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth, height=0.9\textheight]{images/distribution_2.png}
    \caption{Distributions of features (2 of 3).}
    \label{fig:distribution-2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{images/distribution_3.png}
    \caption{Distributions of features (3 of 3).}
    \label{fig:distribution-3}
\end{figure}

\begin{equation}{\dfrac 1 n} \sum^n_{i=1}  \underbrace{\ln ({1+e^{-y_i \theta^\top x_i}})}_{\text{Logistic Loss \ } L(x_i, y_i, \theta_i)}\label{eq:logistic-loss}
\end{equation}

\begin{table}[H]
\centering
\footnotesize
\caption{LIBLINEAR + L2 (Rounded Coefficients)}
\begin{tabular}{|c|p{6.5cm}|}
\hline
\textbf{Class} & \textbf{Top Features (Coefficient)} \\
\hline
0 & \texttt{height\_high} (-3.02), \texttt{height\_medium} (1.75) \\
1 & \texttt{ay\_rms} (-3.37), \texttt{playYears\_high} (-2.33) \\
2 & \texttt{a\_mean} (2.19), \texttt{ax\_rms} (-1.96) \\
3 & \texttt{height\_high} (5.32), \texttt{height\_low} (-3.83) \\
4 & \texttt{playYears\_high} (-2.36), \texttt{ax\_rms} (1.96) \\
5 & \texttt{ax\_mean} (-1.72), \texttt{a\_mean} (-1.62) \\
\hline
\end{tabular}
\label{tab:logreg-liblinear-l2}
\end{table}

\begin{table}[H]
\centering
\footnotesize
\caption{LBFGS + L2 (Rounded Coefficients)}
\begin{tabular}{|c|p{6.5cm}|}
\hline
\textbf{Class} & \textbf{Top Features (Coefficient)} \\
\hline
0 & \texttt{height\_high} (-3.38), \texttt{height\_medium} (1.94) \\
1 & \texttt{ay\_rms} (-3.25), \texttt{playYears\_high} (-2.77) \\
2 & \texttt{a\_mean} (2.21), \texttt{ax\_rms} (-1.96) \\
3 & \texttt{height\_high} (6.77), \texttt{height\_low} (-4.29) \\
4 & \texttt{playYears\_high} (-2.60), \texttt{ax\_rms} (1.95) \\
5 & \texttt{ax\_mean} (-1.72), \texttt{a\_mean} (-1.53) \\
\hline
\end{tabular}
\label{tab:logreg-lbfgs-l2}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/lr/scree_plot.png}
    \caption{Enter Caption}
    \label{fig:pca-scree}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/lr/pca.png}
    \caption{PCA}
    \label{fig:PCA}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/lr/cross-validation - normal.png}
    \caption{Learning curve for Logistic Regression}
    \label{fig:reduced-learning-curve}
\end{figure}




\end{document}

